{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"features_extractor.ipynb","provenance":[{"file_id":"1IouLNRe1l0XXZ2vkGByHLXI8zdUUJ8Do","timestamp":1612384318290}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kfN3N2oSZFre"},"source":["# **Mount**\r\n","\r\n","Mount the google drive directory"]},{"cell_type":"code","metadata":{"id":"k2LZr9WRPhjK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614867976689,"user_tz":-60,"elapsed":72531,"user":{"displayName":"Fabiana Ritorti","photoUrl":"","userId":"01991264164884665531"}},"outputId":"c127a428-f55a-4a9f-9ff4-5d9588cb637a"},"source":["#MIRCV 2021\r\n","from google.colab import drive\r\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6BCvjvYUPw8B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614867996299,"user_tz":-60,"elapsed":16520,"user":{"displayName":"Fabiana Ritorti","photoUrl":"","userId":"01991264164884665531"}},"outputId":"ad902e35-8acc-41a1-e6ab-d260e4382129"},"source":["!pip install opencv-python==4.4.0.46\r\n","!pip install import-ipynb"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting opencv-python==4.4.0.46\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/2d/62eba161d3d713e1720504de1c25d439b02c85159804d9ecead10be5d87e/opencv_python-4.4.0.46-cp37-cp37m-manylinux2014_x86_64.whl (49.5MB)\n","\u001b[K     |████████████████████████████████| 49.5MB 87kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.4.0.46) (1.19.5)\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: opencv-python\n","  Found existing installation: opencv-python 4.1.2.30\n","    Uninstalling opencv-python-4.1.2.30:\n","      Successfully uninstalled opencv-python-4.1.2.30\n","Successfully installed opencv-python-4.4.0.46\n","Collecting import-ipynb\n","  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n","Building wheels for collected packages: import-ipynb\n","  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2976 sha256=4d935281e65bafe4bef6a5671cd08b5c6c84d3901b89fed2a1285ccdf8fdffd9\n","  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n","Successfully built import-ipynb\n","Installing collected packages: import-ipynb\n","Successfully installed import-ipynb-0.1.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F2ewXMLSYgI0"},"source":["# **Unzip**\r\n","Unzip the database if is not yet has been done"]},{"cell_type":"code","metadata":{"id":"2-rzvZCiYgf9","colab":{"base_uri":"https://localhost:8080/","height":58},"executionInfo":{"status":"ok","timestamp":1613125371951,"user_tz":-60,"elapsed":1441,"user":{"displayName":"Fabiana Ritorti","photoUrl":"","userId":"01991264164884665531"}},"outputId":"32f90960-008a-4d39-ec8a-533f95e1497d"},"source":["'''\r\n","# Unzipping dataset\r\n","%cd '/content/gdrive/MyDrive/progetto/dbs/' \r\n","!unzip -q facial_expression.zip\r\n","''' "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n# Unzipping dataset\\n%cd '/content/gdrive/MyDrive/progetto/dbs/' \\n!unzip -q facial_expression.zip\\n\""]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"BiujqgBn5fRN"},"source":["# %cp '/content/gdrive/MyDrive/progetto/dbs/mirflickr25k' '/content/gdrive/MyDrive/progetto/dbs/dataset/Training/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mcqS-vKwYrq8"},"source":["# **Model and Initialization**\r\n","\r\n","Import the preconstructed model(facenet_keras) as well as initialize variables"]},{"cell_type":"code","metadata":{"id":"8txV2ByeP3XE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614868052505,"user_tz":-60,"elapsed":9957,"user":{"displayName":"Fabiana Ritorti","photoUrl":"","userId":"01991264164884665531"}},"outputId":"a90a58d0-5d7a-4720-9c4e-6b2c964e1082"},"source":["import glob\r\n","import os\r\n","import numpy as np\r\n","import PIL\r\n","\r\n","import tensorflow as tf\r\n","from tensorflow.keras import layers as L\r\n","from tensorflow.keras.models import Model, Sequential\r\n","from tensorflow.python.keras.preprocessing import dataset_utils\r\n","from IPython.display import display\r\n","from tqdm.notebook import tqdm\r\n","\r\n","import sklearn\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.metrics import accuracy_score, confusion_matrix\r\n","from sklearn.svm import SVC, LinearSVC\r\n","\r\n","from keras.models import load_model\r\n","\r\n","model = load_model('/content/gdrive/MyDrive/progetto/modello/keras-facenet/model/facenet_keras.h5')\r\n","#model.load_weights('/content/gdrive/MyDrive/progetto/modello/keras-facenet/weights/facenet_keras_weights.h5')\r\n","\r\n","np.random.seed(42)\r\n","tf.random.set_seed(42)\r\n","\r\n","BASE_DIR = '/content/gdrive/MyDrive/progetto/dbs/facial_expression/dataset'\r\n","#BASE_DIR = '/content/gdrive/MyDrive/progetto/dbs/dataset'\r\n","TEST_DATA_DIR = os.path.join(BASE_DIR, 'PublicTest')\r\n","TRAIN_DATA_DIR = os.path.join(BASE_DIR, 'Training')\r\n","\r\n","BATCH_SIZE = 256"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ILqddu9_TIuO"},"source":["# **Dataset**\r\n","For the dataset we collect all the files using the available function of keras image_dataset_from_directory(); by using shuffle=False we mantain the alphanumerical order. We obtain 3589 files divided by 7 classes(suprise,neutral,sad,happy,fear,angry,disgust).\r\n","# **Retriving image paths,labels,classes and image ids**\r\n","Using the utility function index_directory() we obtain the image_paths,labels(the numerical label corrisponding to each classes) of each .jpg in the dataset and class_names. In addition, we collect all the ids by taking the terminal part of each path"]},{"cell_type":"code","metadata":{"id":"tMDEbszfQFM8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614868160905,"user_tz":-60,"elapsed":104201,"user":{"displayName":"Fabiana Ritorti","photoUrl":"","userId":"01991264164884665531"}},"outputId":"fb71158f-cc02-4d17-ede5-38268c06da33"},"source":["dataset = tf.keras.preprocessing.image_dataset_from_directory(\r\n","    TRAIN_DATA_DIR,\r\n","    seed=123,\r\n","    shuffle=False,\r\n","    image_size=(160, 160),\r\n","    batch_size=BATCH_SIZE,\r\n","    color_mode='rgb')\r\n","\r\n","testing_dataset = tf.keras.preprocessing.image_dataset_from_directory(\r\n","    TEST_DATA_DIR,\r\n","    seed=123,\r\n","    shuffle=False,\r\n","    image_size=(160, 160),\r\n","    batch_size=BATCH_SIZE,\r\n","    color_mode='rgb')\r\n","\r\n","testing_image_paths, testing_labels, testing_class_names = dataset_utils.index_directory(TEST_DATA_DIR, labels='inferred', formats='.jpg', shuffle=False)\r\n","testing_image_ids = [path.split('/')[10] for path in testing_image_paths]\r\n","#image_paths, labels, class_names = dataset_utils.index_directory(TRAIN_DATA_DIR, labels='inferred', formats='.jpg', shuffle=False)\r\n","#image_ids = [path.split('/')[9] for path in image_paths]\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 28709 files belonging to 7 classes.\n","Found 3589 files belonging to 7 classes.\n","Found 3589 files belonging to 7 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VGeESBhXShM7"},"source":["# **Extracting features**\r\n","We will use the preconstructed model, by calling its predict() function,to extract the features from the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cgNWOG3OSPp9","executionInfo":{"status":"ok","timestamp":1612807986952,"user_tz":-60,"elapsed":9770642,"user":{"displayName":"MATTEO DESSÌ","photoUrl":"","userId":"04106704574472318930"}},"outputId":"adf7387b-ef33-4c7a-fb58-85ff785f06e2"},"source":["'''\r\n","#testing_features = model.predict(testing_dataset,batch_size=BATCH_SIZE,verbose=1)\r\n","features = model.predict(dataset,batch_size=BATCH_SIZE, verbose=1)\r\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15/15 [==============================] - 1142s 75s/step\n","113/113 [==============================] - 8628s 76s/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wTlSkgdGqNer"},"source":["# **Extracting features from model tuning**\n","We will use the model fine-tuned, by calling its predict() function,to extract the features from the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55hEc62wqM4Z","executionInfo":{"status":"ok","timestamp":1613153447353,"user_tz":-60,"elapsed":1476024,"user":{"displayName":"Fabiana Ritorti","photoUrl":"","userId":"01991264164884665531"}},"outputId":"6ee7b14f-2a43-4ad5-f0a2-9afe73cf4f81"},"source":["'''\n","model_finetune = tf.keras.models.load_model('/content/gdrive/MyDrive/progetto/modello/keras-facenet/model/fine_tuning_all_unfreezed.h5')\n","model_finetune = tf.keras.Model(inputs=model_finetune.input, outputs=model_finetune.get_layer('classifier_hidden_tun').output)\n","\n","#fine_tuned_features = model_finetune.predict(dataset,batch_size=BATCH_SIZE, verbose=1)\n","\n","\n","testing_features_tuned = model_finetune.predict(testing_dataset,batch_size=BATCH_SIZE,verbose=1)\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15/15 [==============================] - 1463s 97s/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7rNBxr8tCFxz"},"source":["# **Extracting features from another model tuning**\n","We will use the model fine-tuned, by calling its predict() function,to extract the features from the dataset"]},{"cell_type":"code","metadata":{"id":"rt9UgiJuCDwH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613233123159,"user_tz":-60,"elapsed":1073655,"user":{"displayName":"Fabiana Ritorti","photoUrl":"","userId":"01991264164884665531"}},"outputId":"217489e0-fdb9-4005-d1c6-f4cd84a19acd"},"source":["'''\n","model_finetune_new = tf.keras.models.load_model('/content/gdrive/MyDrive/progetto/modello/keras-facenet/model/fine_tuning_epochs_2.h5')\n","model_finetune_new = tf.keras.Model(inputs=model_finetune_new.input, outputs=model_finetune_new.get_layer('classifier_hidden').output)\n","\n","#fine_tuned_features_new = model_finetune_new.predict(dataset,batch_size=BATCH_SIZE, verbose=1)\n","\n","\n","testing_features_tuned_new = model_finetune_new.predict(testing_dataset,batch_size=BATCH_SIZE,verbose=1)\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15/15 [==============================] - 1068s 71s/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zuaACtaWPKCZ"},"source":[" # **Extracting features from another model tuning with early stopping**\n","We will use the model fine-tuned, by calling its predict() function,to extract the features from the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wI9U3ZgOPQY8","executionInfo":{"status":"ok","timestamp":1614869445064,"user_tz":-60,"elapsed":1187338,"user":{"displayName":"Fabiana Ritorti","photoUrl":"","userId":"01991264164884665531"}},"outputId":"9ac1524f-ded8-4275-9493-b15575e5cc5f"},"source":["model_finetune_early_stop = tf.keras.models.load_model('/content/gdrive/MyDrive/progetto/modello/keras-facenet/model/fine_tuning_nuovo.h5')\n","model_finetune_early_stop = tf.keras.Model(inputs=model_finetune_early_stop.input, outputs=model_finetune_early_stop.get_layer('classifier_hidden_tun').output)\n","\n","#fine_tuned_features_early_stop = model_finetune_early_stop.predict(dataset,batch_size=BATCH_SIZE, verbose=1)\n","\n","\n","testing_features_tuned_early_stop = model_finetune_early_stop.predict(testing_dataset,batch_size=BATCH_SIZE,verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["15/15 [==============================] - 1175s 78s/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M9hyUDHhS6FK"},"source":["# **Saving as numpy 1**\r\n","We save the ids, features and labels of the images into pickles"]},{"cell_type":"code","metadata":{"id":"VQfugXerSXPU"},"source":["import sklearn\r\n","from sklearn import preprocessing\r\n","'''\r\n","np.save('/content/gdrive/MyDrive/progetto/pickles/image_ids.npy', image_ids)\r\n","np.save('/content/gdrive/MyDrive/progetto/pickles/image_features.npy', features)\r\n","np.save('/content/gdrive/MyDrive/progetto/pickles/image_features_normalized.npy', sklearn.preprocessing.normalize(features))\r\n","np.save('/content/gdrive/MyDrive/progetto/pickles/image_labels.npy', labels)\r\n","\r\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_ids.npy', testing_image_ids)\r\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_features.npy', testing_features)\r\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_features_normalized.npy', sklearn.preprocessing.normalize(testing_features))\r\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_labels.npy', testing_labels)\r\n","\r\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmV-N13615jH"},"source":["'''\r\n","np.save('/content/gdrive/MyDrive/progetto/pickles/image_paths.npy', image_paths)\r\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_paths.npy', testing_image_paths)\r\n","\r\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aNRET_QkQDge"},"source":["# **Saving as numpy 2**\n","We save the ids, features and labels of the images into pickles"]},{"cell_type":"code","metadata":{"id":"aR8GqTjFqdSH"},"source":["'''\n","import sklearn\n","from sklearn import preprocessing\n","\n","\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_ids_finetuning.npy', image_ids)\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_features_finetuning.npy', fine_tuned_features)\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_features_normalized_finetuning.npy', sklearn.preprocessing.normalize(fine_tuned_features))\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_labels_finetuning.npy', labels)\n","\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_ids_finetuning.npy', testing_image_ids)\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_features_finetuning.npy', testing_features_tuned)\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_features_normalized_finetuning.npy', sklearn.preprocessing.normalize(testing_features_tuned))\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_labels_finetuning.npy', testing_labels)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pw7WpyFdqdpg"},"source":["'''\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_paths_finetuning.npy', image_paths)\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_paths_finetuning.npy', testing_image_paths)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"leCTw93iQGYR"},"source":["# **Saving as numpy 3**\n","We save the ids, features and labels of the images into pickles"]},{"cell_type":"code","metadata":{"id":"8_5TYAfpCfUN"},"source":["'''\n","import sklearn\n","from sklearn import preprocessing\n","\n","\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_ids_finetuning_new.npy', image_ids)\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_features_finetuning_new.npy', fine_tuned_features_new)\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_features_normalized_finetuning_new.npy', sklearn.preprocessing.normalize(fine_tuned_features_new))\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_labels_finetuning_new.npy', labels)\n","\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_ids_finetuning_new.npy', testing_image_ids)\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_features_finetuning_new.npy', testing_features_tuned_new)\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_features_normalized_finetuning_new.npy', sklearn.preprocessing.normalize(testing_features_tuned_new))\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_labels_finetuning_new.npy', testing_labels)\n","\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueQ3F65RCkAy"},"source":["'''\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_paths_finetuning_new.npy', image_paths)\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_paths_finetuning_new.npy', testing_image_paths)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AlVcuS8gQIMO"},"source":["# **Saving as numpy 4**\n","We save the ids, features and labels of the images into pickles"]},{"cell_type":"code","metadata":{"id":"gZXAkesmQL-7"},"source":["import sklearn\n","from sklearn import preprocessing\n","\n","\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_ids_finetuning_early_stop.npy', image_ids)\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_features_finetuning_early_stop.npy', fine_tuned_features_early_stop)\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_features_normalized_finetuning_early_stop.npy', sklearn.preprocessing.normalize(fine_tuned_features_early_stop))\n","#np.save('/content/gdrive/MyDrive/progetto/pickles/image_labels_finetuning_early_stop.npy', labels)\n","\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_ids_finetuning_early_stop.npy', testing_image_ids)\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_features_finetuning_early_stop.npy', testing_features_tuned_early_stop)\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_features_normalized_finetuning_early_stop.npy', sklearn.preprocessing.normalize(testing_features_tuned_early_stop))\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_labels_finetuning_early_stop.npy', testing_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CKvixq0BQQ15"},"source":["#np.save('/content/gdrive/MyDrive/progetto/pickles/image_paths_finetuning_early_stop.npy', image_paths)\n","np.save('/content/gdrive/MyDrive/progetto/pickles/testing_image_paths_finetuning_early_stop.npy', testing_image_paths)"],"execution_count":null,"outputs":[]}]}